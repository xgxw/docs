= 面试准备
:toc:
:toclevels: 5
:sectnums:

link:https://draveness.me/golang[go语言技术书, 超赞]

== 技术问题
=== GMP
https://www.cnblogs.com/sunsky303/p/9705727.html

https://juejin.im/post/6844904034449489933

.GMP是Go语言的调度器, 是Go在并发层次上优于其他语言的原因之一(其他还有GC(三色设计+屏障),chanel等). GMP由如下部分组成.
1. G: Goroutinue, 任务对象, 进程控制块的抽象. 包括上下文信息, 现场恢复所需的寄存器信息.
2. M: Machine, 抽象线程. 在用户态运行, 与内核线程对应. 执行时切换到G的寄存器/堆栈信息
3. P: Processer, 抽象处理器. P负责调度协程, 与CPU核对应. P与M绑定形成计算单元.

.其中, G队列绑定在P上, 有如下特性
1. 本地队列. 无锁, 更高效
2. 全局队列. 有锁, 用于不同的P之间负载均衡任务.
3. 本地队列和全局队列会自动进行任务均衡.
4. P一般与系统核数相同, M一般多于P(因为M会出现休眠/阻塞等情况).

image:./assets/gmp.jpg[gmp结构]

.GMP 并发高效的原因
1. GMP调度设计本身
2. 避免陷入内核态. 与多线程相比, G保存了执行时的上下文数据, M执行时只需更换寄存器地址指向即可, 而多线程需要在内核态更改寄存器数据.

image:./assets/gmp-process.jpg[调度过程]

1. 函数执行时创建G, 将其绑定到P上, 如果P本地队列满则放到全局队列
2. 唤醒或从全局队列获取或创建M, 与P绑定, 开始执行G.
3. 进度调度循环: 获取G->执行->清理(goexit)->执行新的G
4. 当本地队列执行完毕, 则从全局队列获取G.

.调度时的一些情况
1. 当G阻塞时(如产生了系统调用), 解绑PM, 将剩余的G和P转移到其他M上执行.当阻塞M执行完毕时, 会尝试获取P, 如果没有获取到, 则G会放到全局队列中.
2. 当某个P/M上所有G执行完毕, 全局队列又没有空闲G时, 会从其他队列中偷取G. 一般是一半.

抢占式调用P 参考 https://tiancaiamao.gitbooks.io/go-internals/content/zh/05.5.html, 可以保证 p 不会饿死, 或某个 p 不会长期占用资源
1. runtime.main 会创建一个额外线程运行 sysmon 函数.
2. sysmon函数会无限循环(休眠时间由 20us->10ms)

==== goroutine
1. 一个 goroutine 大小约为 2kb, 其他语言的thread约为1M左右.
2. goroutine 更简单, 更易于管理.

=== chanel
https://juejin.im/post/6844903821349502990

.chanel可以从两方面理解, 为什么和怎么做
1. 为什么我们需要chanel. go基于goroutinue+chanel实现并发模型. 其设计是参考CSP模型实现的.
  .. CSP(通信顺序模型): 即基于消息顺序控制的并发模型, 各协程间通过消息来处理控制执行, 如阻塞, 如等待.
2. chanel是如何设计的. chanel结构设计, 运行流程 大致如下.

为什么我们需要并发模型. 当摩尔定律失效, 单核的处理能力增速有限, 并发编程开始普及. 基于锁+内核通信的并发编程容易出错(如死锁), 容易降低性能.
后续诞生了 CSP/Actor 等并发编程模型.
// 如果所有进程都是同步的, 我们也不需要chanel了, 直接内存共享即可. 如果单核线程无限快, 我们也不需要并发.

1. CSP 通信顺序模型. 基于消息交互控制. 如Go中 goroutine+chanel 实现的并发控制模型, 通过消息交互数据, 实现控制.
2. Actor 参与者. 一切 每个 Actor 有唯一地址, 进行数据通信, 实现并发控制.
. 参考: https://cloud.tencent.com/developer/article/1349213

chanel 数据结构
{
  // chanel信息
  etype // 元素类型
  buf // 环形缓冲区
  dataqsiz // 缓冲区大小
  closed  // 是否关闭
  // 缓冲区/生产/消费者信息
  sendX/recvX // 发送/接收位置指针,
  sendq/recvq // 发送者等待组, 接收者等待组(链表)
  // 并发管理
  lock // 锁
}

.流程
1. 正常非阻塞流程. send时, 加锁, 从goroutinue copy 到环形缓冲区, recv时, 加锁copy到goroutinue.
2. 当G1发送消息时, 如果缓冲区已满, 则主动调用Go调度器(gopark函数), G1出让资源, 开始等待, 同时G1转换为sudog保存到sendq中等待被唤醒.
  .. 当G2读取消息时, 缓冲区有空位置, 从sendq中唤醒G1, 并将G1放入可执行队列.
3. 当因为没有消息, 消费者阻塞时, 生产者新生产的消息会直接拷贝到 阻塞消费者 的指定地址上(sudog包含该地址), 从而避免chanel锁.

1. 阻塞:
  .. 对于无缓冲区的chan, 只有写入的元素直到被读取后才能继续写入, 否则就一直阻塞.
  .. 对于有缓冲的chan,只有当缓冲满了, 才会阻塞
2. 可以使用 range 或 v,ok<-ch 的方式判断chanel是否关闭.
3. 向已关闭的chanel发送消息会panic, 但是可以从关闭的chanel中读取消息.

.如何优雅的关闭chanel
1. 关闭原则:
  .. 关闭前先检查chanel是否已经关闭
  .. 原则上从生产者端关闭chanel.
2. 使用Once关闭chanel
  func(mc *AStruct) SafeClose() {
    mc.once.Do(func() {
  		close(mc.C)
  	})
  }
3. 单生产者只需在生产端关闭即可. 单消费者可以通过发送信号给生产者来决定是否关闭chanel.
  多生产者/消费者 则需要引入协调者, 通过协调者关闭chanel(某一节点任务完成后通知协调者, 当全部完成则close)

=== 逃逸分析
逃逸分析是一种确认动态指针范围的方法. 可以理解为, 逃逸分析是编译器用于决定变量分配到堆上还是栈上的一种行为.

.手动分配可能导致如下问题
1. 内存浪费, 影响效率. 需要分配在栈上的内存分配到了堆上.
2. 悬挂指针, 即野指针. 指针指向非法的内存地址. 需要分配在堆上的指针分配到了栈上.

.Go逃逸分析特性
1. Go的逃逸分析决定变量应该在堆还是栈上分配内存, 包括使用 new/make 等创建的变量, 所以, 部分情况下无法根据程序确定变量到底分配在哪.
2. 逃逸分析是静态分析. go在编译阶段确立逃逸, 并不是在运行时. 所以, 可以通过查看编译后的分析, 确定变量分配位置.

.Go逃逸分析遵循原则
1. 指向栈的指针不能分配在堆上.
2. 指向栈对象的指针不能在栈对象回收后存活.
3. 具体表现为
  .. 如果函数外部没有引用, 则优先放到栈中.
  .. 如果函数外部存在引用, 则必定放到堆中.
  .. 栈空间不足时, 放到堆上.
  .. 动态类型逃逸. 编译器不知具体类型, 如interface, 无法在栈上开辟指定大小空间.

另外, 变量分配在栈上可以减少GC的压力(标记阶段), 所以合理的分配变量是有必要的.

.FAQ
指针传递确实比值传递效率高么?::
  不一定. 指针传递可以减少底层值的拷贝, 从而提升效率. 但是指针传递会产生逃逸, 会将变量分配到堆中.

=== GC
.GC思想
1. 引用计数法. 当引用计数为0时标记为回收. 可能出现循环引用, 每次赋值需要增加计数.
2. 追踪式垃圾回收. 判断对象是否可达, 一旦发现不可达则标记为删除.

https://segmentfault.com/a/1190000022030353

https://zhuanlan.zhihu.com/p/74853110

.追踪式垃圾回收
1. Mark-And-Sweep. 设置标记位记录对象是否可达. 最开始所有都是0, 如果发现可达则置为1(即是否被指向). 遍历所有变量, 构建可达树, 标记完成后, 标记为0的则会被删除.
2. 三色标记(Go现在使用).

.三色标记. 需要STW
1. 使用三种颜色标记对象. 开始所有对象都是白色.
2. 从程序根结点扫描, 将全局变量和函数栈内的对象标记为灰色.
3. 将灰色对象置为黑色, 将原来灰色变量引用的变量全部置为灰色.
4. 重复第三步, 直到发现没有对象可以置为灰色, 剩余的白色变量则是不可达变量.

.为何三色标记需要STW, 如下举例说明, 现有对象1,2,3. 1是栈上对象(黑色对象), 2被栈上对象引用(灰色对象), 3被2引用.
1. 刚开始, 三个对象都被标记为白色. 第一轮循环, 对象1被标记为黑色
2. 第二轮循环, 对象2被标记为灰色.
3. 当对象1和对象2扫描完成 & 对象3还未被扫描时, 由于未进行STW, 执行程序将对象1指向了对象3, 并且对象2删除了对象3的引用
4. 继续执行GC程序, 由于不会在此扫描黑色对象1的引用, 所以对象3会一直是白色, 不会被标记为黑色, 直到最后被删除.
. 可以看到, 当出现 (1.黑色对象指向了白色对象, 2.灰色对象与白色对象的可达关系被破坏) 时, 就会出现对象丢失的现象.

.屏障机制. 三色标记对象丢失最简单的解决办法就是添加STW, 但是STW降低了GC效率. Go引入了屏障机制, 在无需STW情况下, 破坏上述条件. 思想如下.
1. 强三色: 强制黑色对象不允许引用白色对象. 破坏条件1.
2. 弱三色: 只有白色对象被灰色对象引用, 或者在灰色对象的可达链路上时, 黑色对象才能引用白色对象. 破坏条件2.

.屏障机制实现. 
1. 插入屏障, 强三色. 思想是 当黑色A对象引用B对象时, 将B对象标记为灰色.
  .. 为了保证栈的执行效率, 插入屏障不应用在栈上, 只在堆上生效. 栈容量小但使用频繁, 对栈使用屏障会影响栈的执行效率.
  .. 因为只有堆上使用了插入屏障, 所以结束时需要STW, 在栈上重新扫描一遍.
2. 删除屏障, 弱三色. 思想是 被删除的对象, 如果自身是灰色或白色, 那么被标记为灰色.
  .. 明显可以看到, 此方法会造成一定的误差. 即一个对象即使被删除了最后一个指向它的指针也依旧可以活过这一轮.
  .. 只限定灰/白是因为, 黑色被删除无所谓, 黑色对象引用的所有对象已经被标记为灰色了(在该对象被染为黑的的同时).
3. 混合屏障, 弱三色. _TODO, 理解不全_
  .. GC开始,三色标记正常流程, 标记全局变量和栈变量. 
  .. 将栈上创建的对象都标记为黑色. 从而避免rescan
  .. 被删除的对象标记为灰色. 借鉴删除屏障, 但是避免了栈上的操作.
  .. 被添加的对象标记为灰色. 借鉴插入屏障.

.混合屏障的优势
1. 相较于删除屏障, 混合屏障避免了栈上的操作.
2. 因为栈内存在标记阶段最终都为黑色, 所以无需第二次扫描.

因为内存通常不是业务实践的瓶颈, 所以GC时部分内存未回收完全的代价是可以忍受的.

由于深入与了解Go GC的实现需要去了解的周边知识太多, 如内存分配, 内存管理, 所以这方面还没有去做.

.Go GC流程
1. 清理终止
2. 标记
3. 标记完成
4. 清理

.GC触发
1. 手动触发
2. 定量. 分配的内存到达一定值
3. 定时.

=== mysql
Mysql一般使用 explain/desc 查看sql执行计划, 检查sql问题.

.分库分表
. 横向划分: 我们一般是根据时间划分, 因为时间的局部性, 我们根据时间横向划分. 也可以根据某些字段hash划分.
. 纵向划分: 拆分表结构. 一般都是在划分业务时, 按业务拆分好, 我们现有业务中没有这么做.
. 分库: 不同业务划分不同数据库, 减少数据库压力. 同业务根据情况决定.

.引擎
1. 存储方式不同. innode聚集索引, 即主键索引和数据存在一起, 叶节点同时包含数据. myisam 表结构/索引/数据分三个文件存储, 索引叶节点直接指向地址.
  .. 在不同系统, 文件大小是有限制的.
2. innode支持索引, myisam不支持.
3. myisam 最小粒度锁是表锁, innode最小粒度是行锁

.分布式Mysql
1. 结构: SqlExecer 执行节点, NDB数据存储节点, NDB_Mangerd NDB管理节点.

==== 主从同步
.主从同步用途
1. 读写分离. 主库负责写, 从库负责读. 可解决主库锁表时读的问题.
2. 高可用, 主从切换.

.主从复制原理
1. 主节点创建 log dump 线程, 且主节点加锁
2. 从节点启动线程, 从主节点接收 log dump, 保存在本地的 relay-log.
3. 从节点sql线程, 读取 relay-log, 并执行.

==== 索引
参考 https://tech.meituan.com/2014/06/30/mysql-index.html

Mysql中, 索引分为聚集索引(即主键索引)和非聚集索引.

聚集索引是物理索引, 即数据表的物理存储顺序和索引顺序一致. 非聚集索引是逻辑索引, 可以有多种存储结构.

.索引是为了加快搜索的效率, 所以索引一般有如下几种实现
1. 物理排序. 即主键索引.
2. hash索引(很少使用).
3. 全文索引/倒排索引, 搜素引擎使用很多.
4. B+树索引.

Mysql非聚集索引使用B+树实现. 因为B+树可以加快索引查询效率, 也可以减少索引读取磁盘次数. 下面我们分别从 树的比较和索引本身 解释.

''''
**树**

我们知道, 在一列排序后的数据中, 普遍认为二分法是寻找指定节点的最快方法. 树结构就很适合以分割的方式存储排序后的数据, 并加快查找.

.常用树的比较.
1. 二叉树. 如其名. 好处是可以二分查找数据, 提升查找性能.
  .. 为何需要平衡: 当不平衡时, 可能出现某一链路太长的情况, 从而使二分查找变为单路查找, 影响树的效率. 平衡可以使树的查询效率接近二分查找.
  .. 平衡二叉树通过节点的旋转实现(上下左右节点旋转).
  .. 红黑树通过染色+旋转实现. 复杂度 logN
    ... 染色: 根结点是黑色, 红色节点的两个子节点必须是黑色, 黑色节点的子节点是红色.
    ... 任一节点到叶子节点的简单路径包含同样的黑色节点.
2. B树. 平衡多路查找树. 与二叉树类似, 不过B树是多叉的. B树的所有叶子节点在同一层.
  .. B树的平衡是自下向上的, 当同胞节点没有空间时, 向上分裂父节点.
3. B+树. 与B树类似, 最大的区别是B+树的非叶子节点不保存关键字记录的指针, 只进行数据索引. 各叶子间互相连接.

''''
**索引**

.索引耗时主要是两点, 一个索引本身的查询, 一个是磁盘读取.
1. 磁盘上, 一次最小的存储是一个磁盘块, 一次最小的读取也是一个磁盘块. 一般为4kb.
2. 索引很大, 一般不会也不能全部加载到内存中, 而是存储在硬盘上. 所以, 索引查询有很大的I/O消耗, 所选的数据结构要能有效的降低I/O次数, 同时索引本身的效率也要保证.

.根据如上两个特性, 我们可以分析Mysql为何选用B+树做索引
1. 如果选用二叉树, 一方面因为不断的自平衡需要频繁的访问/修改磁盘块, 一方面二叉树多个节点存在一个磁盘块不够简洁.
2. 如果选用B树. B树的每一个节点都是一个磁盘块大小, 同时每个节点预留一定的空间插入新数据(一般是一半).
3. B+树的诞生. 我们知道, 在索引中, 节点查找时间大于节点存取时间. 在B树中, 父节点页包含数据信息, 会增加I/O次数(因为B树节点同时包含索引关键字和索引数据). B+树将所有的数据都存在叶节点, 非叶节点只存索引关键字, 从而提升每次I/O时数据的有效率, 从而减少I/O次数, 提升索引效率.

简而言之, B+树更合适的原因是, B+树减少了索引查询时的I/O次数. 相较于B树, B+树通过调整数据结构, 使查询时每次I/O更有效率.

''''
**其他**

.索引使用的几个问题
1. 索引遵循最左匹配原则.
  .. 索引列按区分度排序.
  .. mysql会向右匹配到范围查询(>,<等)时停止匹配, 所以将范围查询放在条件的最后边.
  .. 如果有条件 created_at>xx, created_at有索引, 但是实际不会用到. 如果想要用到, 将 created_at 放到order中即可.
2. 索引列不要参与计算. 如不要写 from_unixtime(time)='...', 而是 time=unix_timestamp('...')

.范围查询失效详解
1. 参考 link:https://dev.mysql.com/doc/refman/8.0/en/range-optimization.html[range-optimization]
2. 原文 `If the operator is >, <, >=, <=, !=, <>, BETWEEN, or LIKE, the optimizer uses it but considers no more key parts`, 即当遇到范围查询, 后续条件将不参与索引.

==== MVCC
MVCC, Multiversion Concurrency Control, 多版本并发控制. 主要用于处理读写冲突, 提升数据库在高并发下的处理能力.

MVCC 在 读已提交/可重复读 隔离级别下生效, 通过版本号控制数据正确性. 未提交读不产生冲突, 串行化没有并发.

.MVCC实现
1. MVCC给每一行记录添加了 2/3 个字段
  .. DATA_TRX_ID: 最近更新这条行记录的事务ID
  .. DATA_ROLL_PTR: 指向该行回滚段的指针
  .. DB_ROW_ID: 当表没有主键时, Innodb 生成的隐藏主键.
2. MVCC 通过将同一份数据临时保存多个版本的方式实现并发控制. 其实类似与分布式系统通过版本号确定一致性.

==== 其他

ACID 原子性, 一致性, 隔离性, 持久性

隔离性问题: 脏读, 不可重复读, 幻读. 对应解决方法如下.
隔离性级别: 未提交读, 提交读, 可重复读, 串行化

. 不可重复读指一次事务内多次读取值不同. 可重复读指事务开始时加锁, 如此在事务过程中, 多次读的值就是相同的.

=== redis
redis 是内存数据库, 所以redis主要有两个方向的应用. 数据库, 大量数据的存储和查询. 基于内存, 所以设计/使用上与基于硬盘的不同, 更加注重速度, 结构也更注重简单高效.

单线程+IO多路复用模型(选用系统实现, 如epoll/select).
单线程是因为 redis 的瓶颈不在cpu, 而是内存查找.

redis通过psync执行全量复制或部分复制.

.持久化
1. rdb持久化. 定时将redis数据刷新到磁盘(覆盖更新), 异步操作. 性能更好, 但要可以承担最近几秒/几分钟的数据损失. 使用 psync 备份到磁盘.
2. aof持久化. 将redis操作日志以追加的方式写入文件. 类似mysql binlog思想.

==== sds
redis 中的key和字符串value使用的都是sds结构.

sds可以减少变量需要重新分配空间的次数(通过使用内部的free从而减少重新分配次数)

.类似go中的切片, 有三个字段组成: 
1. buf: 字节数组
2. free: 数组中未使用的数量
3. len: 数组中已使用的数量
4. sds 以C风格的 '\0' 作为字符串末尾

==== 跳跃表
跳跃表类似树, 通过将数据集中部分节点作为索引节点提到上一层实现索引. redis通过

在 zset(有序集) 结构中, 底层使用跳跃表实现.

与平衡树相比, 跳跃表实现更为简单, 也不需要rebalance.

==== 压缩列表
redis中 哈希表/列表/有序集合 底层皆使用了压缩列表.

.压缩列表好处
1. 在一定的时间复杂度下, 节省内存. 使用hash实现比压缩列表更占用内存(map底层会有些key是空的).
2. 减少内存碎片. 因为压缩列表物理上时一连串的内存地址.

压缩列表是由一系列特殊编码的内存块构成的列表, 结构如下
`| zlbytes | zltail | zllen | entry1 | entry2 |  ...   | entryN | zlend |`

1. zlbytes: 整个 ziplist 占用的内存字节数. 重分配时使用.
2. zltail: 到达 ziplist 表尾节点的偏移量.
3. zllen: ziplist 中节点的数量.
4. zlend: 末尾标识符.
5. entry结构: `| pre_entry_length | encoding | length | content |`
  .. pre_entry_length: 记录了前一个节点的长度. 可以通过这个值跳转到上一个节点
  .. encoding: content 编码方式. 分为整数/字符数组
  .. length: 本节点长度.

==== 分布式方案
1. 主从高可用. 分为主从节点, 主从节点间同步数据, 用于保证主节点挂掉后有备用节点.
2. 分片式. redis采用主从式结构, 通过zookerper选取master节点, master通过hash算法决定数据存放在哪个节点.
  .. redis采用的不是一致性hash, 而是 `hash(crc(key))`.
  .. 当新增/删节点时, Redis槽位发生变化, 会发生数据的迁移. 访问要迁移的key可能发生重定向.

=== 锁
参考: https://tech.meituan.com/2018/11/15/java-lock.html

悲观锁和乐观锁::
  悲观锁在获取数据时直接加锁, 乐观锁只在更新数据时加锁.
  乐观锁一般采用 CAS(compare and swap, 比较并交换) 实现.

可重入锁::
  **同一线程**可以多次获取同一把锁, 而无需重新获得锁. 可重入锁适合同一线程多个函数需要多次加锁, 如需要锁的递归过程, 或此函数调用的函数需要同样的锁(如更新密码需要调用验证密码, 都需要锁定密码).

自旋锁和阻塞锁::
  自旋锁是指线程反复检查锁变量是否可用,并不释放CPU等资源. 自旋锁适合等待时间较短的情况, 引入自旋锁为了避免线程颠簸.

公平锁和非公平锁::
  公平锁是指优先把锁给等待时间最长的线程, 非公平锁是指先抢到锁的线程优先拿锁.
  公平锁发生线程上下文切换的概率更大, 非公平锁可能造成线程饥饿.

惊群效应::
  在多个请求等待获取锁时, 一旦占有锁的线程释放后, 所有等待方都同时被唤醒, 但是绝大多数的抢占都是不必要的.

=== kafka
kafka是一个着重于吞吐量设计的流式消息队列.
与rabbitmq等消息队列相比, kafka吞吐量更高, 但是消息可靠性, 功能不如rabbitmq.

.kafka broker 结构: https://zhuanlan.zhihu.com/p/71093510
1. Broker. 消息中间件处理结点, 一个 Kafka 节点就是一个 broker, 多个 broker 可以组成一个 Kafka 集群.
2. Topic. 一类消息.
3. Partition. topic 物理上的分组.
4. segment. 每个Partition由多个segment file组成.
5. offset. Partition中消息的序号.
6. 消息. kafka最小单位.

.segment file
1. 由两部分组成: index file和data file, 后缀分别为 .index/.log index记录消息的offset+物理偏移地址, data记录具体的信息.
2. segment file是按照offset分段的, 如 0-1000 在第一个文件中, 命名为 0..0.index/log, 1000-2000在第二个文件中, 命名为 0..2000.index/log. 文件值最大为long值的大小, 即64位二进制, 19位字符串大小, 前缀0填充.
3. 分段是为了方便查找offset.

按照功能, 消息队列分为: 生产者, 消费者, 消息中间件节点, zookeeper集群(保证一致性)

kafka 通过 zookeeper 实现集群管理.

分区以文件夹形式存储数据, 分区有索引加快检索.

==== offset
https://www.jianshu.com/p/449074d97daf

.kafka中有两种offset
1. Current Offset,本地offset. 消费者端保存的offset.
2. Committed Offset, 服务端offset. Broker端保存的offset, 表示Consumer已经确认消费过的消息的序号.

如果使用 Current Offset, 当消费者 reblance或挂掉重启后, offset位置将丢失.
如果使用 Committed Offset, reblance或消费者重启不影响offset记录, 因为是记录在服务端的.

.消费者组
1. 在消费者组中, Group Coordinator 负责 Consumer Group的管理, 各Consumer的offset管理, Consumer元数据(id等) 等.
2. 在消费者组中, 一个partition只能固定的交给一个消费者组中的一个消费者消费, 因此kafka以 `groupid-topic-partition -> offset` 的方式保存offset.
3. kafka将offset存在topic `__consumers_offsets` 中, 读取时通过 Offsets cache 查询 offset. 更新offset时首先发消息到topic中, 然后更新cache. 

auto.offset.reset 配置, 表示如果Kafka中没有存储对应的offset信息的话, 消费者从何处开始消费消息(可指定 earliest(最早)/latest(最新)/none(直接抛异常))

==== 顺序消费
kafka保证单Partition内消费是有序的, 多Partition消费不一定是有序的(如果要保证多partition有序, 则p1阻塞后, p2也会阻塞(要有序), 会影响kafka的吞吐性).

.kafka 消息分区策略
1. 发送函数签名 kafka.send(topic, partition, key).
2. 如果指定partition, 则发送到指定patition.
3. 如果key为null, 则根据topic名获取上次计算分区时使用的一个整数并加一取模.
4. 如果key不为null, 则根据key hash值选择分区.

.当要求消费顺序时.
1. 只创建一个Partition. 但此时kafka高吞吐量的优势无法很好的体现.
2. 当多个Partition时, 同一组业务数据设置相同的key, kafka会将相同key的数据放入一个partition. 如用户的一次购买过程.
3. 借助订单状态, 将消息与数据对比, 状态正确则处理, 不正确则扔回延迟队列(适合基本有序的数据, 无序程度太高不合适)

pravega 大数据流式存储
pulsar 大数据 流批统一 消息队列, bookeeper 存储海量数据且高效(分层)

=== epoll
https://www.cnblogs.com/aspirant/p/9166944.html

epoll是Linux内核的可扩展I/O事件通知机制, epoll让需要大量操作文件描述符的程序得以发挥更优异的性能.

典型使用场景是 redis/nginx, 这些场景下通常有海量客户端与服务器保持连接, 但是每一时刻通常只有几百几千个活跃连接, 很需要使用I/O复用提升效率.

.I/O 事件通知机制有如下几种实现
1. 忙查询. 当阻塞时, 线程隔一段事件扫描一次所有I/O事件.
2. select 无差别查询. 当I/O事件发生, 轮询所有监听的事件.
3. epoll. 当I/O事件发生时, 同时知道那些事件发生了, 只轮询发生I/O的事件.

epoll解决I/O多路复用的问题. I/O多路复用就通过一种机制, 可以监视多个描述符, 一旦某个描述符就绪(一般是读就绪或者写就绪), 能够通知程序进行相应的读写操作.

Linux 原来使用select处理I/O事件通知, 当事件发生时, select轮询所有监听的I/O事件, 复杂度O(N).
epoll 只监听其中发生事件的 I/O通知, 复杂度为 O(K) 或 O(1)

1. epoll 在epoll_ctl函数(create)中, 创建时就会把所有的fd拷贝进内核, 而select是在每次调用时, 都会发生将fd集合由用户态拷贝到内核态.
2. epoll 为每个fd指定一个回调函数, 通过回调确定具体的fd. select/poll 通过监听文件描述符实现, 只知道有事件发生.
3. select 由于单个进程能够监听的文件描述符有最大限制(系统可调), 且select使用轮询, 所以监听句柄有上限. 而epoll则无此限制.

=== 网络
.OSI七层模型
1. 应用层. 应用级. 如 http/ftp/pop3(邮件), 针对不同软件的不同协议.
2. 表示层. 数据格式转换. 如 ssl/tls.
3. 会话层. 建立/管理/维护/关闭通信连接, 如 rpc.
4. 传输层. 管理两个节点间的数据传输. 有 tcp/udp.
5. 网络层. 地址管理和路由选择. 如 IP/ICMP.
6. 链路层. 物理层面上互联节点之间数据的传送. 如 PPP.
7. 物理层. 将数据的 0/1 转换为 高低电平或脉冲信号.

.三次握手
1. syn_sen状态. 建立链接, client 发送Syn(seq=i)包 到server.
2. syn_recv状态. 服务器回应, 服务器回应 Ack(seq=i+1) 到client, 并且发送Syn(seq=j)包给client
3. established状态. 客户端回应, clent 发送Ack(seq=j+1) 到服务器, 链接建立完成.

.四次挥手. 链接关闭也可以是服务端发起关闭.
1. 客户端发送 FIN报文 给服务端
2. 服务端收到报文, 回复ACK给客户端, 同时服务端告诉进程关闭链接
3. 服务端内部处理完毕后, 发送 FIN 给客户端.
4. 客户端发送 ACK 给服务端.

==== HTTP2
https://developers.google.com/web/fundamentals/performance/http2?hl=zh-cn

http2解决了http1存在的问题, 主要是连接问题(tcp长链接)和传输问题(数据格式, 传输格式:二进制).

.架构
. 数据流：已建立的连接内的双向字节流，可以承载一条或多条消息。
. 消息：与逻辑请求或响应消息对应的完整的一系列帧。
. 帧：HTTP/2 通信的最小单位，每个帧都包含帧头，至少也会标识出当前帧所属的数据流。

.关系
. 所有通信都在一个 TCP 连接上完成，此连接可以承载任意数量的双向数据流。
. 每个数据流都有一个唯一的标识符和可选的优先级信息，用于承载双向消息。
. 每条消息都是一条逻辑 HTTP 消息（例如请求或响应），包含一个或多个帧。
. 帧是最小的通信单位，承载着特定类型的数据，例如 HTTP 标头、消息负载等等。 来自不同数据流的帧可以交错发送，然后再根据每个帧头的数据流标识符重新组装。

消息是最小的逻辑交互单位, 即 Request/Response 都是基于消息交互, 消息由 header/data Frames 组成.
但是一次物理通信最小的单位是帧, C/S 发送数据最小的单位是帧. 如一个消息有多个 data Frame, C/S 每次通信发一个 Frame, C/S 端会整理 data Frame.

===== http/sse
sse 是指 websocket 等技术, 用于解决在浏览器内的应用层次上, 页面与服务端通信的问题. 
js 可以控制sse, 但不能控制http.
服务端也可以通过隧道随时向页面发送消息, 而http2的服务端发送只是提前加载 css/js 等资源, 是浏览器层面的数据.

http 是基于浏览器层面考虑的, sse 是基于应用程序层面考虑的.

sse 底层是基于 http 的.

=== rpc
rpc即远程服务调用, 是一个概念/技术规范. grpc是一种实现, http+restful也可以视为一种实现.

rpc解决在实行微服务架构后, 众多微服务之间的调用, 治理的问题.

.rpc主要由如下模块组成
1. 服务治理.
2. 数据传输格式, 序列化与反序列化.
3. 通信协议. http2/socket/tcp/udp
  .. udp不支持可靠传输, 使用udp时需要rpc框架作出相应处理.
4. 异常处理

.关于http和rpc
1. http也可以视为远程服务调用的一种, 解决两个应用之间的相互调用. 此时, 相较于服务间直接http调用, rpc的优势在于
  .. rpc使用场景做了优化.
    .. rpc 支持服务治理(重启/扩容等), 连接池, 服务注册与发现, 负载均衡, 限流, 重试等功能.
    .. 使用上将路由接口化, 规范化.
  .. rpc的数据传输更高效. rpc改进了数据格式, 数据序列化, 相较于http报文更加简介. 如grpc的protobuf.
  .. http的优点: 可读性强, 使用广泛.
2. http也可以单纯作为rpc通讯协议的选择之一, 其他可选的协议还有 socket, tcp/udp等.
. tcp是传输层协议(第四层), http/rpc 都是应用层协议(五层模型). 在七层模型中, rpc是会话层, http是应用层.

===== grpc
grpc底层使用 http2 作为通信传输协议, 但相较于直接使用http, grpc的protobuf格式与序列化/反序列化技术更为高效, 以及作为rpc功能更丰富.

grpc 本身不支持负载均衡/服务发现, 但是预留了相关接口. 可以通过 etcd/envoy 等技术实现类似功能

.protobuf
1. 优点: 序列化/反序列化快(具体源码未研究), 向后兼容, 二进制框架, 带压缩功能, 支持http2.
2. 缺点: 不是http. 表象来看就是, 可视化, 浏览器友好, 阅读友好等.

=== slice
切片数据结构
{
  byte*     array;      // actual data,                   指针 指向数组的某个位置
  uintgo    len;        // number of elements,            表示从指针指向位置 向后取多少个元素
  uintgo    cap;        // allocated number of elements,  表示该数组的最大长度
}

slice步长 -> 新slice 是在原数组/slice(地址) 上取一段地址, 不会发生拷贝, 开辟新地址等操作.

数组/切片区别:
1. 数组是值类型, 切片是引用类型.
2. 数组初始化时确定长度, 后续不可更改.
3. array 的长度是Type的一部分, 即 [10]int 与 [20]int 是不同的.

=== map
1. hash方法. 追求目的: 减少碰撞, 完美分配key.
2. 存储结构: 将hash值分散到连续地址上.
  .. hash冲突常用解决方法: 冲突元素置于一个数组中, map查找时先找到地址, 然后遍历List.

.hash函数常用思想
1. 求模.
  .. 一般使用素数求模, 因为素数求模相比合数碰撞更小.
2. 位操作配合其他方式. 具体方法不再讨论.

.map key 遍历无序的原因
1. 当map扩容时, map的key会重新进行hash, 如此遍历时顺序肯定发生变化.
2. go1.0 之后, map key 遍历时, 会添加一个随机数, 从随机位置开始遍历, 所以每次遍历起始位置不同, 顺序也自然不同. 不过相对顺序还是一致的, 如 `0-1-2 -> 1-2-0` (遍历内存地址顺序)

=== gin/net/http
1. 性能提升: 框架相较于原生 net/http 包, 路由管理性能提升很大.
2. 功能提升: 中间件, 返回数据reader, context参数, bind方法等.

.gin/iris/echo 等选择
1. 功能/用法类似, 具体没有深入研究过. 速度也差不了多少. iris 据说功能更全面, gin路由更强, echo更简单. 具体选型还是看团队原有框架吧, 或者选一个自己喜欢, 顺手的.

.路由匹配
1. 思想: 使用树的方式, 采取前缀匹配(包含 完全匹配/模糊匹配/正则匹配(可选) 几种模式)
2. iris 使用 muxie 库实现, 具体没有研究.

=== 负载均衡
1. Load Banlance Proxy 模式. 代理模式, 由指定节点实现负载均衡. 该类节点可能是由特定设计的机器承担的.
2. Client Load Banlance. 客户端负责负载均衡策略.

=== context
关闭方法: ctx.Done(), ctx.WithCanel() 返回canel方法

WithValue, WithDeadLine(时间点关闭), WithTimeOut() 时间间隔后关闭.

WithValue() -> calueCtx, 结构
{
  Context // Context, 所以直接可以取其字段, 包括k/v.
  key,value interface{}   // WithValue/Value() 写/取值时, 会判断key是否comparable(即是否可以被当作key)
}

=== HLL
HyperLogLog redis 基数计数算法.

标准误差 0.81, 通常使用多次HLL算法减小误差. 数据越随机, 试验次数越多(即数据量越大), 准确性越高.

具体原理参考 自己写的博客.

=== gorm
. DB, gorm对数据库的抽象. 负责与用户交互, 以及与数据库交互.
. Scope, 构建查询条件(Conditions), 执行SQL, 调起回调函数.
. CallBack, 负责CURD具体的执行逻辑. 具体的Conditions处理, db交互
  通过 Scope 执行的.

gorm(Go Object Relational Mapping, Go 对象关系映射).

=== 内存对齐
字段的不同排列方式可能造成所占大小不同.
起因是底层架构中, 内存对齐的原因. 内存对齐是为了加快访问, 一般采用2的指数次方对齐.
起因是 内存访问远远低于CPU周期, 造价也低于计算资源.

内存对齐是指CPU对内存的对齐访问, 所谓对齐访问, 包括两个方面: 起始位置+对齐字节值.
起始位置规则如下: 如果 sizeof(type)==N, 那么起始位置要能被N整除.
- 当访问1byte的数据时, 起始位置要能被1整除(就是有空闲就可以放)
- 当访问2byte的数据时, 起始位置要能被2整除
- 当访问4byte的数据时, 起始位置要能被4整除

对齐字节值规则如下(C语言, Go也适用):
1. 数据成员对齐规则:
    - 如果该成员是自带类型如int, char, double等, 那么 `内存对齐参数 = 该类型在内存中所占的字节数`
    - 如果该成员是自定义类型(如struct), 那么 `内存对齐参数 = 该类型内内存对齐参数最大的成员`
    - 如果自行设置了 内存对齐参数=i字节, 类中最大成员内存对齐参数为j, 那么 `内存对齐参数 = min(i, j)`
2. 整体对齐规则: 在数据成员完成各自对齐之后, 自定义类型(如struct)本身也要进行对齐. 整体内存对齐参数是 **内存对齐参数的k倍.**
    - 重点在 整体内存对齐参数的值, 而不是k的值. 之所以是k倍, 是因为结构体中类型数量和位置是不确定的, 所以k也是不确定的. 具体看后续介绍
3. 类中第一个数据成员放在offset为0的位置; 对于其他的数据成员(假设该数据成员内存对齐参数为k), 他们放置的起始位置offset应该是 `min(k,n)` 的整数倍

注意, 这里再次强调下内存对齐是为了保证CPU用最少的内存访问次数读取对象的值.


没有对齐时, 一次访问可能需要两次读取. 非对齐存储时, 一个数据可能存在两行上(offset发生变化), 则需要多一次读取.
.举例: 假设要读取2byte的数据 `int16类型`
 - 假设内存对齐: 只要 `startAddr%2==0` 即可. 如起始地址为 0x00, 那么16bit只需要从0x00连续读取16位即可.
 - 假设内存没有对齐
  - 如果 `startAddr/16<=1`, 既数据在同一offset内, 则一次读取也可以读出全部值
  - 如果 `startAddr/16>1`, 假设起始地址是0x18H(十进制24), 所以第一个字节存储在 offset为0的 A3, 最后一个字节存储在A0, 但是偏移量不同. 又因为offset只能是4的倍数, 所以第一次读取offset=0的 (A0-A3), 第二次读取 offset=1的 (A0-A3)', 然后拼接两段值得到2byte数据.

=== 缓存
缓存穿透. 恶意访问或非法id造成, 无数次击穿缓存访问数据库.

缓存雪崩. 缓存集体失效.

.缓存设置经验
1. 程序访问具有局部性. 空间局部性和时间局部性. 一个被访问的位置很可能被再次访问(缓存设置), 相邻的地址也可能被访问(底层设计, 高速/低速缓冲器).

LRU: 最远最少使用.
redis LRU 不是完全LRU的, 而是随机选择一定大小的块, 按LRU规则筛选. 可配置.

=== Go内存原理

=== 内存管理
.流程
1. 从系统申请一大块地址, 目的是减少系统调用的次数.
2. 将申请到的内存按特定大小切分为小块, 构成链表. 一般按照8的倍数切分. 为对象分配内存时, 只需从链表中取出一段即可.
3. 回收对象时, 直接将内存归还给链表
4. 闲置内存过多时, go尝试将内存归还给系统.

.内存块分类
1. span: 多个地址连续的页组成, 大块内存, go内部管理.
2. object, 将span切分为小块内存后的链表, 每个小块存储一个对象.

.内存分配器
1. cache: 每个线程绑定一个cache. 无锁分配, 线程私有, 保证线程高效.
2. central: 为所有cache提供span资源. 负责均衡各cache的object资源.
3. heap: 管理闲置span, 负责向系统申请内存. 负责均衡不同规格的span.

为何使用虚拟地址?::
  内存分配和GC回收都需要连续地址(如分配时都是 起始地址+长度), 虚拟地址可以保证这一点.

----
页所属 span 指针数组   GC 标记位图         用户内存分配区域
+-----------------+-------------------+---------------------------------------+
| spans 512MB     | bitmap 32GB       | arena 512GB                           |
+-----------------+-------------------+---------------------------------------+
spans_mapped         bitmap_mapped     arena_start   arena_used      arena_end
----

== 项目相关
=== 储值业务设计
.储值业务划分
1. 核心功能: 储值, 消费, 退款. Order
2. 支付服务. Payment
3. 商户/用户服务. 商户信息, 储值规则, 收银员信息等.
4. 增值服务. 添加到微信卡包, 微信模板消息, 邮件等.
5. 数据统计. 对账服务, 统计服务等.

.支付的可靠性
1. 支付流程的可靠性. 由于Order/Payment分别属于两个服务, 所以需要事务. 我们主要通过两个措施保证一致性.
  .. 重试和消息补偿. 当消息消费失败, 会将其加载到延迟队列, 重新消费, 有些服务则是另起协程, 一般是每 1/5/10s 重试, 全部失败则通知关单.
  .. 超时关单. 业务方负责超时检测. 将消息放入延迟队列(chanel或消息队列), 当订单超时时触发关单操作.
2. 对账检查. 保底措施, 通过对账检测数据的正确性.
  .. 对账常用指标: 应收, 实收, 退款, 手续费/丰润, 交易笔数等
  .. 业务指标: 门店层次:用户留存率, 回头率. 公司运营层次: 商户交易数, 作弊统计(根据交易频次/每单交易额/是否异地, 从而决定限制交易或限额)

一致性其他实现参考: https://cloud.tencent.com/developer/article/1041507

.服务可用性
1. 监控+日志. CPU/内存/消息队列 超额/异常报警, mysql慢任务统计.
2. 数据灾备, 服务异地多活, 主从数据库.
3. grpc+etcd 实现服务自动注册, 自动负载均衡, k8s 自动扩容.
4. 降级. 通过etcd配置某些功能降级. 暂时未实现自动熔断.
5. 使用缓存减少数据库压力, 重要数据启动时预加载到缓存, 缓存/数据库双写

=== 数据统计设计
1. go+kafka 流式计算
2. spark+hive 等

.丢单检查
1. 检查订单是否存在
  .. 主要通过 桶+map 实现, 首先将第三方数据加载到缓存, 然后流式读取内部订单数据对比.
  .. 考虑到 map 的hash规则, 一般使用 订单号前缀或时间戳分桶.
  .. 如果三方数据是流式的, 则可以使用流式的方式处理, 更加简单.
2. 检查金额是否一致
3. 检查总金额是否一致.

== 系统设计

=== 分布式事务方案
尽量避免分布式事务.

1. Mysql XA 事务. 通过增加事务管理器.
2. RocketMQ 事务消息
3. 自己实现
  .. 最大努力交付机制. 事件补偿+超时回滚机制. 如 order/payment 分布式事务处理, 使用 消息补偿+超时关单 方式保证事务, 通过订单状态确定事务状态.
  .. 2PC: 协调者+参与者. 两阶段提交. 准备阶段和提交阶段.
  .. 3PC: 协调者+参与者. 三阶段提交. 准备, 预提交, 提交.
  .. TCC, Try-Confirm-Cancel. 在业务层次保证事务.

=== 分布式锁实现
==== redis分布式锁
redis 中有命令 SETNX, 当设置成功时返回1, 失败时返回0. 因此特性常用来做分布式锁.

也可以通过 SET 设置. SET 签名如下: `SET key value [EX seconds] [PX milliseconds] [NX|XX]`. 
EX即秒, PX即毫秒, 都用来设置有效时间. NX表示当键不存在时才设置成功, XX表示当键存在时才设置成功.

WARNING: SETNX/SETEX/PSETEX 后续版本可能被剔除, 统一使用SET实现, 且 NX/EX 无法直接设置过期时间, 需要通过 `EXPIRE key seconds` 设置key的过期时间, 非原子操作.

.SETNX使用注意事项
1. 多线程之间key冲突. 因为多个线程执行同一个函数时, 使用预设的固定值会对其他线程产生干扰, 所以一般会加上线程的标志或者加随机值.
2. 上锁后无法正常解锁. 线程设置锁后, 程序异常/崩溃导致无法锁无法解除, 所以要给锁设置过期时间.
3. 函数在锁过期后仍未执行完毕. 添加监控者, 单起一个线程监控加锁线程, 当线程还存在则延期锁, 否则删除或者等待锁过期. 参考redisson实现.
4. 分布式系统中, 加锁后某台机器挂掉的情况. 如锁hash到A机器上, 但是A机器挂了. 暂时无解.

=== 延迟队列实现
1. redis zset. 通过设置score实现.
2. rabbitmq ttl(存活时间)+dxl(死信队列)实现.
3. 类似netty的时间轮调度算法. 设置一组时间key, 然后将队列挂在key的队列上, 然后定时调起列表上的任务. key也可以是环形的时间轮, 将触发事件hash后挂载到轮上.
4. go timer 延时实现. 最小堆方式.

https://www.cyhone.com/articles/analysis-of-golang-timer/


=== 一致性hash
一致性hash解决分布式节点中数据/访问负载均衡的问题. 一致性hash解决两个问题, 1.如何尽量保证访问均匀的分散在各个节点. 2.如何尽量保证增删节点时对每个节点上缓存的原先数据影响最小.

最简单的方式是通过hash取模, 将访问分散在各个节点. 这种方法最大的问题是, 当新增/删除节点时, 所有的访问都会重新hash, 导致每个节点上的缓存数据都可能有大量的失效和更新.

简单的改进是采用环形hash, 节点均匀的分布在环上, 访问时按顺时针访问环上的下一个节点. 这个方案的问题是, 当增删节点后, 容易出现负载不均衡的情况.

再次改进, 将真实节点改为虚拟节点, 环上分布诸多个虚拟节点, 多个虚拟节点映射到一个真实节点上, 如此便可有效的避免增删节点后导致缓存大量更新的情况, 也保证访问分配的平衡性.

== 算法
算法参考 leetcode/编程之美 等资料

== 其他问题
Go使用过程中不爽的地方::
  1. 没有多态. 同一函数参数不同时写起来很不优雅.
  2. 没有注解. AOP(面向切面)编程写起来不爽. 如事务传递的实现.
  3. error 处理. 无法更好的获取code. 替代方案可以参考 grpc/error.

